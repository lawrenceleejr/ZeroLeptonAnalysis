---+++ Possible types for systematic uncertainties in HistFitter

We discussed already in the first part of this tutorial [[HistFitterTutorial#Systematic_uncertainties]] that HistFitter allows to implement systematic uncertainties as different types, depending on the effect the systematic uncertainty should have. In particular we introduced the types

   * =overallSys= if a systematic uncertainty only varies the scale of the nominal histogram
   * =histoSys= if the systematic uncertainty varies both the shape and the scale of a nominal histogram
   * =normHistoSys= if the systematic uncertainty only varies the shape of a nominal histogram

HistFitter provides much more possibilities to implement systematic uncertainties. The most common types (more types, in particular also user defined systematic uncertainties, exist) are:

| *Basic systematic methods in HistFactory* ||
| =overallSys= | uncertainty of the global normalization, not affecting the shape |
| =histoSys= | correlated uncertainty of shape and normalization |
| =shapeSys= | uncertainty of statistical nature applied to a sum of samples, bin by bin |
| *Additional systematic methods in HistFitter* ||
| =overallNormSys= | =overallSys= constrained to conserve total event count in a list of region(s) |
| =normHistoSys= | =histoSys= constrained to conserve total event count in a list of region(s) |
| =normHistoSysOneSide= | one-sided =normHistoSys= uncertainty built from tree-based |
| ^ | or weight-based inputs|
| =normHistoSysOneSideSym= | symmetrized =normHistoSysOneSide= |
| =overallHistoSys= | factorized normalization shape and uncertainty; described with |
| ^ | =overallSys= and =histoSys= respectively |
| =overallNormHistoSys= | =overallHistoSys= in which the shape uncertainty is modeled with a =normHistoSys= |
| ^ | and the global normalization uncertainty is modeled with an =overallSys= |
| =shapeStat= | =shapeSys= applied to an individual sample |

(This table is extracted from the HistFitter paper: http://arxiv.org/abs/1410.1280 )



---+++ Which systematics type should be used?

Given these rather large amount of systematics types, one could get confused which type should be used in a certain situation. Alex Koutsman made a nice flowchart to illustrate the decision process.

<picture>


---+++ "Normalized" systematic uncertainties and the transfer factor approach

A basic ingredient in many analyses is the extrapolation from control to signal or validation regions for estimated a major background in a semi-data-driven way. The idea is to fix a background (model) in a fit to data in control regions that were specifically designed for the specific backgrounds. Using then Monte Carlo simulations, this background model is extrapolated to signal regions in order to obtain a background estimate in them. This extrapolation from control to signal regions is only possible if the kinematic variables used in the extrapolation are well described by Monte Carlo simulation. This assumption can be check in validation regions kinematically close to the signal regions, but usually containing a lower signal contamination.

This analysis strategy is illustrated in the two plots taken from the HistFitter paper (http://arxiv.org/abs/1410.1280) - look into this paper if you are not familiar to this concept of extrapolation!

A schematic view of an analysis strategy with multiple control, validation and signal regions. All regions can have single- or multiple bins, as illustrated by the dashed lines. The extrapolation from the control to the signal regions is verified in the validation regions that lie in the extrapolation phase space.

A typical analysis strategy flow with HistFitter.



---++++ The concept of transfer factors

The extrapolation from control to signal (validation) regions itself is described by so-called transfer factors, where we need one transfer factor for every sample (i.e. background process) and every extrapolation. The transfer factor from a control to a signal (validation) region is defined as ratio of the Monte Carlo yields in the signal (validation) region to the yield in the control region.

---+++++ Mathematical description


For the mathematical description we cite here the HistFitter paper:

"
A key underlying concept of the fitting procedure is the implicit use of ratios of expected event counts, 
called _transfer factors_, or TFs, for each simulated background process between each SR and CR.

The normalized background predictions used in fit are:

%BEGINLATEX%
\begin{eqnarray}
N_p({\rm CR}, {\rm est.}) &=& \mu_{p}\times {\rm MC}_p({\rm CR},{\rm init.}) \,, \nonumber \\
N_p({\rm SR}, {\rm est.}) &=& \mu_{p}\times {\rm MC}_p({\rm SR},{\rm init.}) \,,
\end{eqnarray}
%ENDLATEX%

where %$N_p({\rm CR}, {\rm est.})$% (%$N_p({\rm SR}, {\rm est.})$%) is the CR (SR) background estimate for each simulated physics processes %$p$% 
considered in the analysis, %${\rm MC}_p({\rm CR}, {\rm init.})$% (%${\rm MC}_p({\rm SR}, {\rm init.})$%) is the initial estimate 
as obtained from the MC simulation, and %$\mu_{p}$% is the normalization factor as obtained in the fit to data.

In the fit, the background estimate(s) is (are) typically driven by the statistics in the CR(s).  
Define %$N_p({\rm CR}, {\rm fit})$% as the fitted number of events for process %$p$% in the CR.
Then equivalently:

%BEGINLATEX%
\begin{eqnarray}
N_p({\rm SR}, {\rm est.}) &=& \mu_{p} \times {\rm MC}_p({\rm SR},{\rm init.}) \nonumber \\
                     &\equiv& N_p({\rm CR}, {\rm fit}) \times \left [\frac{{\rm MC}_p({\rm SR}, {\rm init.})}{{\rm MC}_p({\rm CR}, {\rm init.})}\right ] \,.
\end{eqnarray}
%ENDLATEX%

The ratio appearing in the square brackets of Eq. (2) is defined as the transfer factor TF.
The two notation are equivalent in terms of the fit: there is a normalization quantity, derived from the fit to data, multiplied
by a fixed constant.
In other words, albeit the fit uses Eq. (1) internally, 
%$N_p({\rm SR}, {\rm est.})$% can always be interpreted as a TF multiplied by the fitted number of background events in the CR.

An important feature of the TF approach is that systematic uncertainties on the predicted background processes 
can be (partially) canceled in the extrapolation; a virtue of the ratio of MC estimates.
The total uncertainty on the number of background events in the SR is then a combination of the statistical uncertainties in the CR(s) and the residual systematic uncertainties of the extrapolation. 
For this reason, CRs are sometimes defined by somewhat loose selection criteria, 
in order to increase CR data event statistics without significantly increasing residual uncertainties in the TFs,
which in turn reduces the extrapolation uncertainties to the SR.
"



---++++ The usage of "normalized" systematic uncertainties

If using the transfer factor approach systematic uncertainties on the predicated background processes might be partially cancel, if having a similar impact in control and signal (validation) regions. This well-known virtue of Monte Carlo ratios can help in reducing the total uncertainty on background estimates in the signal regions, under the assumption that the control regions were carefully chosen.

Exactly this transfer factor approach is meant when using systematic uncertainties with _Norm_ in the name.

To use these types, we first need to explain to HistFitter which regions should be taken as _normalization regions_. The normalization are used to normalize the systematic uncertainties to. Let's take e.g. an up variation of a nominal histogram. We could e.g. define the nominal histogram as normalization region. When doing so, we would normalize our up histogram such that it has the same integral as our nominal histogram. Effectively this means, that the up variation will only differ in shape to the nominal histogram, but no longer has a different normalization (== scale).

Usually, we take as normalization regions all the control regions that the analysis has, but the choice really depends on your needs - you should thus discuss this with your reviewers.

Getting a bit more concrete - let's have again a look at the example configuration file =analysis/tutorial/MyConfigExample.py= which already reflects a quite complex analysis.

Open this file:

nedit analysis/tutorial/MyConfigExample.py &

There are three systematic uncertainties implemented as =histoSys=: JES, KtScaleWZ, KtScaleTop

First we want to see the impact of these systematic uncertainties in the signal region =SS=. In order to see this run a background-only fit with the validation regions turned on (=doValidation=True=, line 50 in the configuration file):

HistFitter.py -t -w -f analysis/tutorial/MyConfigExample.py

Create from this a systematic table:

SysTable.py -w results/MyConfigExample/BkgOnly_combined_NormalMeasurement_model_afterFit.root -c SS_metmeff2Jet -o systable_SS.tex

In the configuration file =analysis/tutorial/MyConfigExample.py= you find the following lines:

topSample.setNormRegions([("SLWR","nJet"),("SLTR","nJet")])
wzSample.setNormRegions([("SLWR","nJet"),("SLTR","nJet")])
bgSample.setNormRegions([("SLWR","nJet"),("SLTR","nJet")])

This tells HistFitter to use the two control regions =SLWR= and =SLTR= as normalization regions, to be used for the backgrounds =Top=, =WZ= and =BG=.

Now switch in the config file the three uncertainties JES, KtScaleWZ, KtScaleTop to the type =overallNormHistoSys=. (You will see that these lines already exist in the configuration file as commented lines, uncomment those and comment the original ones.)

Running again:

HistFitter.py -t -w -f analysis/tutorial/MyConfigExample.py
SysTable.py -w results/MyConfigExample/BkgOnly_combined_NormalMeasurement_model_afterFit.root -c SS_metmeff2Jet -o systable_SS_2.tex

Now compare the two tables:

kompare systable_SS.tex  systable_SS_2.tex

Indeed you find that the three uncertainties JES, KtScaleWZ, KtScaleTop are reduced if using the systematics type =overallNormHistoSys= with normalization regions.


---++++ Behind the scenes: internal construction of "normalized" systematic uncertainties

The construction of the normalized histograms contains multiple steps and this thus not necessarily very intuitive. For this reason, we illustrate specifically for the systematics type =overallNormHistoSys= on how the original input histograms are modified. 

Various normalization steps are carried out as follows:
   * Temporary histograms are built containing the sum of the yields in all normalization regions (this is done for the nominal, the up and the down histogram).
   * First normalization step: Up and down temporary histograms are scaled to the yield in the nominal temporary histogram.
   * The obtained scaling factor is applied to the original up and down histograms.
   * If the systematics type contains an =overall= in the name - second normalization step: Up and down histograms are scaled to the yields in the nominal histogram. The resulting histograms are internally used with an =histoSys= type, the scaling factor with a =overallSys= type.


*Caution:* Care must be taken when applying =Norm= type systematics, as only samples that carry a normalization factor =mu_...= in the fit
can be used. Otherwise the uncertainty on the total event count or normalization is not taken into account for this sample, which
can lead to underestimation of the sample uncertainty in the signal region.

*Exercise:* 

In the data file data/MyConfigExample.root we have stored both the original, unnormalized histograms and the normalized histograms. Open this file:

root data/MyConfigExample.root

And list the histograms with =.ls=. Histograms like =hWZNom_SLTR_obs_nJet= or =hWZJESHigh_SLTR_obs_nJet= are the unnormalized, original histograms (syntax: =h=+ sample + Nom or systematics + region + =_obs_= + variable). Histograms like =hTopJESHigh_SLWRSLTRNorm= or =hTopNom_SLWRSLTRNorm= (so with a region string identifying the normalization region =SLWRSLTR= and an additional =Norm=) are the temporary histograms used in the first bullet point above. Finally, histograms like =hWZJESHigh_SLTR_obs_nJetNorm= are the final, normalized histograms that are used further (they differ from the name of the original histogram only by a =Norm= at the end).






